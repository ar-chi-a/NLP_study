임베딩 (Word Embedding)


1. Word2Vec

원핫-인코딩 한계점 : 연산이 낭비되어 모델 학습에 불리하게 적용 -> 밀집 벡터로 해결
                    단어 의미를 담지 못함 -> 분석을 효과적으로 수행할 수 없음. 아직 해결 못함
                    
단어 유사성으로 이 벡터 표현의 질을 측정
더 적은 비용으로 높은 정확도 개선

많은 양 데이터를 활용한 단순 모델이 적은 데이터를 복잡한 모델을 적용한 것보다 성능이 높음
많은 양 데이터를 복잡한 모델을 학습 -> 단어의 분산표형을 사용
                   
유사단어 간에는 거리가 가까운 경항이 있고, 단어는 다양한 유사도를 가짐

LSA보다 뛰어난 선형 정규성
LDA는 데이터 양이 많을 수록 많은 연산이 필요
복잡도가 LDA 보다 현저하게 낮음

CBOW 모델 :
Skip Gram 모델 :

낮은 연산 복잡도
높은 정확도

결과를 어떻게 할 것인가, 성능지표는 어떻게 볼 것인가 : 의미적 문항과 문법적 문항으로 테스트

영어로 시각화
https://ronxin.github.io/wevi/

[실습] Word2Vec
입력 문장 토큰화(사용하지 않는 품사 제거)
가중치 = 파라미터 = 단어벡터 초기화
원핫 인코딩
epoch 만큼 반복 : 중심단어, 문맥단어 추출 등


2. GloVe

 기존 임베딩의 문제점 : 
 LSA : 단어 유추 문제에 좋지 않은 성능
 word2vec : 단어 유추에는 좋은 성능. 학습 데이터에서 관찰되는 단어 사용 통계정보를 활용하지 않음 

 위 두 가지의 강점을 합쳐서 만든 것이 GloVe
 벡터간의 유추문제에 좋은 성능을 보이면서 말뭉치 전체의 통계 정보를 반영이 핵심 목표
 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 등장확률 로그값이 되도록 목적함수를 정의
 
 단어-문맥 행렬
  단어간 co-occurrence 행렬을 생성
  
 
 최적화 문제 : 가장 최적으로 만족시키고자 설계한 성능을 특별히 목적함수로 정의
              다목적 함수 : 하나 이상의 세무 성능들로 구성된 목적함수
 


3. [실습] Fast Text

from gensim.models import FastText



4. 문맥적 단어 임베딩 - Elmo, BERT 등

RNN, Transformer 에 의해 자연어 처리가 크게 변함
BERT를 기반으로 파생된 모델
GPT 계열의 초대형 모델

문맥에 따라 벡터 생성 -> 같은 단어여도 문맥에 따라 다른 벡터가 생성될 수 있음
같은 단어라도 문맥에 따라 다른 방식으로 표현

문장을 입력 받아 각 단어에 대한 representation을 산출
문맥에 의존적인 단어를 산출하는 feature

























