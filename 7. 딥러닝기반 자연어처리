

1. 딥러닝 기초

비용함수 : 예측값과 실제값의 차이를 최소로 하는 함수
역전파 : 오차 역전파법, 경사하강법


2. RNN

유닛간의 연결이 순환적 구조
시계열 특징을 모델링 할 수 있음

주식과 같은 연속적인 시계열 데이터에 적합 - 언어 모델링, 기계번역, 음성인식 등

1) RNN - one to many
2) RNN - many to one
3) RNN - many to one stacking
4) RNN - many to many
5) RNN - many to many bidirectional

RNN 문제점 : 장기의존성(long term dependencty) 문제

LSTM(long short-term memory) 장기 단기 메모리 네트워크는 장기적인 종속성을 학습할 수 있는 특수한 종류의 RNN

LSTM 구조 - cell state, 이정 정보를 유지하기 용이
           input gate를 활용하여 cell state에 대한 정보를 잊거나 추가할 수 있음
           sigmoid를 활용하여 추가, 유지 결정
           
           
GRU (gated recurrent unit) : LSTM셀의 간소화된 버전, 2014


[실습] RNN을 사용한 텍스트 분류
텐서플로우 공홈 자료 활용



3. Elmo (embedding from language model) 2018

사전 훈련된 언어 모델을 사용 
문맥에 따라 임베딩
순방향, 역방향으로 예측하는 biLM으로 사전 훈련
순방향은 다음단어를 역방향은 이전단어를 예측

'언어모델 사전학습'과 '자연어처리 Task적용' 2단계로 수행

char CNN : 글자단위로 CNN을 적용하여 임베딩 함으로써, 문매고가 상관없이 단어 안에 포함된 서브단어도 임베딩 가능
            OOV에 견고
          
          
4. CNN


5. Seq2Seq

many to many 모델 중 하나
encoder-decoder 구조
기계번역에 많이 사용

인코더 - 컨테스트 벡터 - 디코더

학습 - teacher forcing

[실습] Seq2Seq
텐서플로우 공홈 자료 활용


6. Seq2Seq (attention)

attention mechanism
입력 시퀀스의 길이에 상관없이 단일 컨텍스트 벡터로 표현하여 정보 병목 현상이 발생
디코더에서 예측시 인코에서 집중할 정보를 더 고려하여 예측

어텐션 스코어 - 어텐션 분포


[실습] Seq2Seq (attention)
텐서플로우 공홈 자료 활용
어텐션을 사용한 인공 신경망 기계 번역



7. Transformer

CNN이나 RNN을 사용하지 않음

-long-term dependency problem : (RNN은 정보사이의 거리가 멀때 이를 이용 못함) attention mechanism 으로 해결
-parallelization : (RNN은 병렬화 불가능했음) 이를 행렬 연산 병렬화로 해결

1) 인코더
 - feed forward, multi-head attention
 - tranformer 인코더 : multi-head attention
    self attention - scaled dot-product attention
    
 
(어려워서 스킵)

[실습] Transformer
텐서플로우 예제로 연습


8. BERT (2018)

-pre-training of deep bodirectional transformer for ~~
-대용량의 unlabeled data로 모델을 미리 학습 시킨 후 특정 task를 가지고 tranfer learning을 하는 모델

-fine-tuning : tranfer learning, fine-tunning(사전에 학습됨 모델의 파라미터를 task에 맞추어 정교하게 조정하여 활용)

-masked language model(MLM)
-next sentence prediction(NSP)






