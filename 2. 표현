표현(Representation)
 단어를 숫자화 하는 과정
 
[단어의 표현 - 원핫인코딩, 유사도 계산] 
1. 원핫-인코딩(one-hot-encoding)
 한계점 : 차원 크기의 문제, 의미를 담지 못하는 문제, 자연어 처리 유사성 판단-코사인 유사도, 
         원핫 벡터간 코사인 유사도는 모두 0 -> 따라서 의미를 분간하기 어려움
         벡터로 표현한 단어 차원이 너무 큼 -> 연산이 낭비되어 모델 학습에 불리하게 적용
         단어 의미를 담지 못함 -> 분석을 효과적으로 수행할 수 없음
         
2. 유사도 계산(similarlity)
 유클리디언 거리, 한계점 : 
 자카드 유사도(Jaccard index) : 문서 혹은 문장간 유사도 측정 (겹치는 토큰의 비율)
 코사인 유사도(cosine similarity) : 두 벡터간 각(코사인 유사도)를 이용한 유사도 측정
 
3. 단어 임베딩(word embedding)
 원핫인코딩 단점 해결
 단어 임베딩은 단어의 의미를 간직하는 밀집 벡터(dense vector)로 표현하는 방법
 벡터가 공간에 꽉차 있음
 새로운 단어 추가시 차원을 추가할 필요가 없음 -> 차원을 줄일 수 있음 -> 연산을 줄일 수 있음
 차원이 커지는 것을 밀집 벡터로 해결
 단어들이 벡터 위에서 어디에 위치하는 것이 맞을까? -> 단어를 벡터로 표현하는 명확한 방법이 존재하지 않음
 
 밀집 벡터를 만드는 방법:
  분포가설 -> 같은 문맥에서 등장하는 단어는 유사한 의미를 지닌다 -> 같은 문맥에 등장하는 단어를 더 가까이 표현
  
 local representiation : 단어 그 자체만 보고 값을 매핑하여 표현
 distributed " : 단어를 표현하기 위해 주변을 참고
 
 
 [실습] 구글 코랩에서 진행


[단어의 표현 - TF-IDF, N-gram]

1. TF-IDF (term frequency-inverse document frequency)
 단어빈도 - 역문서 빈도
 TDM 내 각 단어의 중요성을 가중치로 표현, TDM을 사용하는 것보다 더 정확하게 문서 비교가 가능
 tfidf 수식
 DF
 IDF : df(t)의 역수, 문서와 상관없이 많이 등장하는 단어의 가중치를 낮춤. 
       내문서에 많이 등장하고 타문서에 많이 등장하지 않는 것이 IDF가 높음
 IDf에 로그를 사용하는 이유 :            
   
 tf-idf 계산절차 : 토큰 index 생성 -> tf 계산 -> idf 계산 -> tf-idf 계산
 
2. n-Gram 이란?
 복수개의 단어를 보느냐에 따라 uni-, bi-, tri- 등으로 구분
 제한적으로 문맥을 표현할 수 있음
 -> 점점 앞 뒤 단어들이 붙기 떄문
 
 n의 크기는 trade-off 문제 : n을 너무 크게 선택하면 n-gram이 unique 할 확률이 높아 등장수가 낮을 확률이 높음
                            n을 너무 작게 선택하면 카운트는 잘되지만 정확도가 떨어질 수 있음. 
                            n은 최대 5를 넘지 않도록 
 
 적용분야에 맞는 코퍼스의 수집 : 분야에 따라 단어들의 확률분포는 다름. 
                               분야에 적합한 코퍼스를 사용하면 모델 성능이 높아질 수 있음
 
  [실습] 단어의 표현 TF-IDF, N-gram
    -> 깃허브 발견!! 깃허브 코드 자료 참고
  
  




