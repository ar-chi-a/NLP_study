[핵심키워드 추출 (TF-IDF)]

중요한 단어를 추출 -> 단어의 중요성을 어떻게 판단할 것인가
필요성 : 대량 데이터 처리 가능, 추출의 일관성, 실시간 분석 가능단

통계적 접근
-단어빈도 : 문서를 단어모음으로 간주, 빈도를 활용
-연어/동시발생 : N-gram 같은 통계기법을 활용, 
               연어(연이어 함께 자주 등장하는 단어 묶음), 
               동시발생(동일 코퍼스 내에 함께 등장하는 단어 묶음, 연어와 달리 단어가 인접할 필요 없음) 
 
1. TF-IDF 활용 핵심 키워드 추출
토큰화 -> TF-IDF계산 -> TF-IDF score가 높은 순으로 추출

[실습] 핵심키워드 추출(TF-IDF)
Mecab 활용
from konlpy.tag import Mecab

gensim 활용
from gensim.models import TfidfModel
from gensim.corpora import Dictionary


2. TextRank 활용 핵심키워드 추출 
그래프 생성->중요도 계산
Google PageRank를 기초로함
관련 논문 - TextRank:Bringing Order into Texts
           그래프 기반 랭킹 모델, 키워크와 문장 추출을 위한 비지도 학습 방법을 제안
           그래프의 각 vertex의 중요도를 결정하는 방법(그래프 기반 랭킹 알고리즘)
           그래프속 vertex(노드)의 중요도를 결정하는 방법 -> 하나의 vertex가 다른 vertex에 연결되면 투표 또는 추천
           많은 득표한 vertex가 중요 vertex
           undirected 그래프
  (논문 내용에 따른 설명 - 어렵다)         
           
[실습] 핵심키워드 추출(TextRank)
1) 행렬활용
-unique한 토큰 목록 생성 -> 그래프 생성(weighted edge 계산) -> 각 노드의 score계산 - > 핵심 단어 추츨

(어렵다..)


3. 문서요약 (Document Summarizer)
문서에서 중요한 문장을 자동으로 추출하는 과정


[실습] 문서요약 (Luhn Summarizer)


4. 문서요약 (Text Rank)
텍스트 내 다양한 문장 사이의 관계 강도가 결정되고, 이를 역순으로 정렬하여 텍스트를 요약
유사성은 content overlap함수로 측정

잘 작동하는 이유?

키위드 추출 vs 문서요약 : 윈도가 이동하며 그래프 생성 vs 모든 문장간 유사도를 기준으로 그래프 생성
text rank 과정 : 행렬로 계산하기

[실습] 문서요약 (Text Rank)
-mecab설치(필요시)
-matrix활용


5. 토픽모델링 (LSA)

문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나로, 텍스트 본문의 숨겨진 의미구조를 발견하기 위해 사용
토픽을 추출된 키워들의 분포로 나타냄으로써 텍스트 내의 구조를 파악

활용 : 뉴스기사로부터 토픽 분석, SNS 상에서의 주요 이슈를 추출, 미래 핵심 기술과 이슈를 발견하고 트랜드 분석, 토픽으로 사회와 시대를 이해

잠재의미분석(LSA) : 동일한 의미를 공유하는 단어들은 같은 텍스트에서 발생 한다고 가정하는 벡터기반 방법.
                  TDM 내에 잠재된 의미를 이끌어 내는 방법
                  
 LSA를 활용하여 의미를 보존하며 밀집벡터 생성가능
 
 잠재 의미 분석 절차 : 문서, 단어 -> TDM 생성 -> 특이값 분해(SVD) -> 토픽 모델링 -> 단어간, 문서간, 문서-단어간 유사도 분석
                                            (토픽모델링)          ->              (벡터 활용)

확률적 잠재 의미 분석(pLSA) : 특이값 분해가 아닌 확률적 방법을 사용
                             문서는 여러 주제로 구성되어 있고, 각 주제는 단어 집합으로 구성
                             
                     한계 : 오버피팅 되기 쉬움
          
          
6. 토픽모델링(LDA) 
잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)

주어진 문서에 대해 어떤 주제가 존재하는지에 대한 확률모형(토픽모델링)
토픽별 단어의 분포, 문서별 토픽의 분포를 추정

잠재(latent) : 감춰진 파라미터 

단어교환성

n-gram방식으로 LDA의 교환성 가정을 확장
 
generative / discriminative model
 간접적 / 직접적
 데이터 범주의 분포 / 결정경계를 학습
  
깁스 샘플링(Gibbs sampling)

  
어렵다..


[실습] 토픽모델링(LDA)

import pyLDAvis
import pyLDAvis.sklearn


7. 문서분류(NBC) document classification

문서를 사전에 구성된 그룹으로 분류하는 모델
분류모델 - 나이브 베이즈 분류 : 학습 데이터에서 추출한 이미 알고 있는 사전확률을 바탕으로 사후확률을 계산하여 분류
                              성능 개선을 위한 방법 : 불용어 처리 등
                              
        - 서포트 벡터 머신 : 제한된 양의 데이터를 처리할때 좋은 성능을 보이는 분류 알고리즘. 
                            주어진 그룹에 속하는 벡터와 그렇지 않은 벡터 간 분류를 결정
                           
        - 딥러닝 : CNN RNN Transformer 기반 모델. 잘 태깅된 데이터가 필수
        
1) 베이즈 분류기
 -데이터의 조건부 확률에 기반한 분류
 -범주형 자료에만 적용가능
 -대량의 데이터가 필요
 -exact(조건이 많으면 계산 어려움) / naive(독립변수가 많을때)
                           

예제) 사기 재무보고 예측
 사기/ 정직
 이전 법적문제 유/무
 
2) 나이브 베이즈 분류
 -특성들 사이의 독립을 가정
 -문서를 여러 범주(스팸 등)로 판단하는 대중적 방법
 -감성분석에도 활용. 감정라벨이 부착된 학습용 데이터가 필요
 
 -베이즈 정리 : 두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리
               사전확률, 우도, 사후확률
               사전확률로부터 사후확률을 구할 수 있음
               
[실습] 문서분류 (NBC)












 


